{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from util.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(28996, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (1): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (2): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (3): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (4): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (5): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (6): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (7): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (8): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (9): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (10): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "      (11): BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertModel.from_pretrained('/data/medg/misc/phuongpm/biobert_v1.1_pubmed')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('/data/medg/misc/phuongpm/biobert_v1.1_pubmed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/data/medg/misc/phuongpm/\" + \"test1.0.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = JsonDataset(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = list(data.json_to_plain(remove_notfound=True, doc_ent=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "input0 = sample_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "document, query, candidates, answers = input0['p'], input0['q'], input0['c'], input0['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_to_plain_doc(document):\n",
    "    tokens = document.split()\n",
    "    for i, t in enumerate(tokens):\n",
    "        if t.startswith('@entity'):\n",
    "            tokens[i] = t.replace(\"@entity\",\"\").replace(\"_\", \" \")\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'marginal zone lymphoma'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"more than meets the eye : the ‘ pink salmon patch ’ summary @entityocular_adnexal_lymphomas account for 1 – 2 % of all @entitynon-hodgkin_'_s_lymphomas . conjunctiva is the primary site of involvement in one - third of cases . we present a case of a 47 - year - old hispanic woman who presented with @entityleft_eye_itching and @entityirritation associated with a @entitypainless_pink_mass . @entityphysical_examination revealed the presence of a ‘ pink salmon - patch ’ involving her left medial conjunctiva . @entityorbital_ct showed a @entitysubcentimeter_left_preseptal_soft_tissue_density . @entitybiopsy revealed a @entitydense_subepithelial_lymphoid_infiltrate comprised predominantly of b cells that did not coexpress cd5 or cd43 . these @entityfindings were consistent with @entityb_-_cell_marginal_zone_lymphoma . @entityfurther_staging_assessment did not reveal @entitydisseminated_disease . she had @entitystage_1e_extranodal_marginal_zone_lymphoma as per ann arbor staging system . she received @entityexternal_beam_radiotherapy to her left eye with complete resolution of the @entitylymphoma in 2 months and continues to remain tumour free at 8 - month follow - up . she will be followed up closely for development of any local ( @entityunilateral_or_contralateral_eye_)_or_systemic_recurrence in the long run . background this paper reports about @entityocular_marginal_zone_lymphoma . although this entity is uncommon , its presenting features are similar to the more commonly seen conditions . hence , its early diagnosis may be missed . since this tumour has very good response to radiation , especially when it is localised , early recognition and prompt treatment is imperative . clinicians should be aware of this @entitycondition and request a @entityconjunctival_biopsy when in doubt . also , since there remains a @entityhigh_risk_of_progression_to_systemic_lymphoma in the long run , these patients should be monitored closely . case presentation a 47 - year - old hispanic woman was seen in the ophthalmology clinic for a 3 - week history of a @entitypainless_pink_mass_on_the_inner_aspect_of_her_left_eye associated with @entityitching and @entityirritation . there was no @entityheadache , @entitypainful_eye_movements , @entityblurry_vision or @entitydouble_vision . she did not have watering of the eye , @entityredness or @entityphotophobia . there was no history of use of @entitytopical_ophthalmic_medications . she had no @entitysystemic_complaints . she had no history of @entitysexually_transmitted_diseases . her medical history was significant for @entitypoorly_controlled_diabetes_mellitus and @entitymigraine . she denied smoking , alcohol or illicit drug abuse . her family history was significant for the presence of @entitydiabetes_mellitus in her mother . on @entityexamination , her visual acuity was 20 / 20 in each eye . pupils were normal in size and bilaterally equal reacting to light . ocular motility and @entityintraocular_pressures were normal . @entityphysical_examination_of_the_left_eye showed an @entityirregular_‘_pink_salmon - patch ’ involving the superomedial aspect of the conjunctiva with @entityextension_into_the_inferior_fornix ( figure 1a , b ) . @entityright_eye_examination was normal . @entitydilated_fundus_examination showed normal macula , vessels and periphery bilaterally . there were no @entitypreauricular_,_submandibular_or_cervical_lymph_nodes palpable . @entityrest_of_the_physical_examinations was unremarkable . investigations @entityct_of_the_orbit_with_contrast showed a @entitysubcentimeter_left_preseptal_soft_tissue_density ( figure 2 ) . @entitybrain_ct was unremarkable . @entitybiopsy of the @entityconjunctival_mass showed a @entitydense_subepithelial_lymphoid_infiltrate_comprised_predominantly_of_b_cells ( figure 3a – c ) that did not coexpress cd5 or cd43 . clonal rearrangement of @entityimmunoglobulin_heavy_chain_gene was detected by pcr . these @entityfindings were consistent with a @entityb_-_cell_type , @entitymarginal_zone_lymphoma . results of @entitylaboratory_tests were normal including a @entitycomplete_blood_count and @entityserum_lactate_dehydrogenase_(_ldh_)_level . @entitychest_,_abdomen_and_pelvis_cts were unremarkable . 18 - @entityfluorodeoxyglucose_-_positron_emission_tomography_(_pet_)_/_ct revealed @entityenhanced_metabolism_in_the_left_medial_conjunctiva . there was no evidence of @entitydisseminated_disease . @entitybone_marrow_examination did not reveal any evidence of @entitylymphoma . the patient tested negative for @entitychlamydia_psittaci . the patient was thus diagnosed with @entityleft_conjunctival_extranodal_marginal_zone_lymphoma ( emzl ) , stage 1e according to the ann arbor classification . @entityleft_inferior_conjunctival_mass biopsy : @entitydense_infiltration_of_monotonous_cells_beneath_the_conjunctival_epithelium and even in perivascular areas . cells are arranged in @entitynodular_pattern ( h & amp ; @entitye_stain × 40 ) . @entitydense_infiltration_of_monotonous_cells_beneath_the_conjunctival_epithelium ( h & amp ; @entitye_stain × 100 ) . small to @entityintermediate_-_sized_lymphocytes with @entitydispersed_cytoplasm_without_nucleoli ( h & amp ; e stain × 400 ) . differential diagnosis the key clinical feature is evaluation of a @entitysalmon_pink_mass_over_the_bulbar_conjunctiva . @entitydifferential_diagnoses include @entitynodular_scleritis , @entitychronic_follicular_conjunctivitis , @entityfibrovascular_pterygium , @entitybenign_tumours such as @entitysquamous_papilloma , @entityreactive_lymphoid_hyperplasia and @entitymalignant_tumours such as @entityocular_adnexal_lymphomas ( oals ) and @entityamelanotic_melanoma . @entitynodular_scleritis is associated with @entitysystemic_diseases in 50 % of cases . these include @entitysystemic_lupus_erythematosus , @entityrheumatoid_arthritis , @entitysjogren_'s_syndrome , @entitywegener_'s_granulomatosis and @entityspondyloarthropathies . it usually presents with @entityredness and a @entitysevere_constant_boring_pain that radiates to the face , worsens at night and with eye movement . this patient had a @entitypainless_mass which would make this diagnosis less likely . however , @entityautoimmune_workup including @entityana , @entityds_-_dna , ra factor , @entityanti-ccp_,_c_-_anca , p - anca , @entityss_-_a_,_ss_-_b_and_anti-rnp may be sent if in doubt . @entitypterygium typically starts medially on the nasal conjunctiva and extends laterally onto the cornea in a wedge - shaped fashion that is its distinguishing feature . it is usually soft in consistency , regular in shape and bilateral . this is in contrast to @entityneoplastic_lesions that are often more vascular , @entityirregular and unilateral . the @entitymass in this case was @entityirregular and did not involve the cornea making @entitypterygium less likely . treatment involves primary excision of the @entitymass with @entityconjunctival_autograft . @entitychronic_follicular_conjunctivitis occurs most commonly due to @entitychlamydia_trachomatis_infection . it is a @entitysexually_transmitted_disease . clinical features include @entitymucous_discharge , @entitycrusting_of_lashes_,_conjunctival_hyperaemia and @entitychemosis . @entityfollicular_reaction ( cobblestone appearance ) is a key feature and typically involves the bulbar conjunctiva and semilunar folds , which was not seen in this patient . owing to the @entityhigh_prevalence_of_associated_genital_tract_disease , @entitysystemic_therapy is preferred . confirmed cases may be treated with @entitydoxycycline 100 mg twice daily for 10 days or @entityazithromycin 1 g single dose . it is thus clear based on the above discussion that initial differential diagnosis of a @entityconjunctival_mass is broad and @entitybiopsy is essential to rule out @entitymalignant_aetiologies . @entitytreatment the patient was treated with @entitylocalised_external_beam_radiotherapy ( ebrt ) . the total dose given was 3060 cgy with @entityfractions of 180 cgy / day . a @entitytungsten_eye_shield was used to cover the patient ’ s eye ( except for the area of interest ) during @entitytherapy to reduce the risk of @entityradiation_keratopathy , @entitycataract and @entityretinopathy . outcome and follow - up on follow - up after 2 months , the patient did not report any @entityitching or @entityirritation_of_her_left_eye . @entityslit_-_lamp_examination revealed complete resolution of the @entitysalmon_-_patch seen previously . a @entityrepeat_ct_of_the_orbits demonstrated complete resolution of the @entityleft_conjunctival_lesion ( figure 4 ) . the patient continues to follow - up without any @entityvisual_complaints and remains tumour free at 8 - month follow - up now . she will have long - term follow - up and will be monitored for development of any local ( @entityunilateral_or_contralateral_eye_)_or_systemic_recurrence . @entityprevious_studies have shown that the rate of progression to @entitysystemic_lymphoma is 38 % at 5 years and 79 % at 10 years of the initial diagnosis . discussion oals account for 1 – 2 % of all @entitynon-hodgkin_'_s_lymphomas . conjunctiva is the primary site of involvement in 20 – 33 % of cases followed by orbit in 46 – 74 % , eyelid in 5 – 20 % , lacrimal sac in 7.5 % and caruncle in 2.5 % . multiple site and @entitybilateral_involvement is seen in 10 – 20 % of patients . ocular adnexa may also be the site of relapse in @entityotherwise_systemic_lymphomas . emzl of @entitymucosa_-_associated_lymphoid_tissue ( malt ) type is the most common subtype , followed by follicular and @entitydiffuse_large_b_-_cell_lymphoma . c. @entitypsittaci , @entityhelicobacter_pylori , @entityborrelia_burgdorferi and @entityhepatitis_c_virus have been linked to @entitymalt_lymphomas . emzl usually occurs in fifth to seventh decade and has a female predominance . clinical features are often non-specific . it usually presents as a @entitysalmon_-_pink_nodular_patch involving the bulbar conjunctiva . features such as @entityrapid_growth , @entityinvasion_of_surrounding_tissues , @entityulceration and presence of feeder vessels support a diagnosis of @entitylymphoma . in a review by white et al , 7 common presenting @entitysymptoms included @entitymass in 48 % , @entityswelling in 45 % , followed by @entitydiplopia , @entityptosis and @entityproptosis . imaging plays an important role in detecting small , @entityoccult_or_multifocal_lesions . @entityct and / or @entitymri_of_the_brain_and_orbit help to assess size and location of the @entitytumour . they , however , do not differentiate benign from @entitymalignant_lesions . they also help in assessing patient 's response to @entitytherapy as described in the present case . @entityb_-_scan_ultrasonography is more sensitive in @entitydetecting_small_extrascleral_lesions . however , clinical presentation and @entityimaging do not help with the definitive diagnosis . confirmation depends on histopathology , immunophenotype and molecular genetics . @entityimmunophenotypic_analysis for @entityb_-_cell and @entityt_-_cell_markers , heavy and light chain restriction , cd5 , @entitycd10 , cd23 , @entitycyclin_d1 and bcl - 2 should be performed . @entityflow_cytometry helps with assessing quantitative data . mantle cell and @entitymarginal_cell_tumours appear to be histologically similar but can be distinguished based on their @entitydifferential_expression_of_cd5 . @entitymolecular_genetic_analysis for @entitygene_rearrangements_of_the_igg heavy chain can determine clonality . additional staging should follow including @entitylaboratory_workup ( complete blood count and @entityldh_level ) , @entitychest_x-ray , @entityct_of_the_chest_,_abdomen_and_pelvis , and a @entitybone_marrow_biopsy . a @entitypet_scan is more sensitive than @entityct in determining @entitydistant_disease . all patients should be appropriately staged using the conventional ann arbor or the @entityrecent_american_joint_committee on @entitycancer_tumor - node - @entitymetastasis ( tnm - ajcc ) staging system . according to ann arbor staging , patients can be classified as stage i , ii , iii or iv . stage i denotes oal in a localisd area . stage ii indicates @entitylymphoma_in_two_separate_areas , an @entityaffected_lymph_node_or_organ and a @entitysecond_affected_area , and both affected areas are confined to above or below the diaphragm . stage iii refers to lymphomas that have spread to involve both sides of the diaphragm ( including 1 organ or area near the lymph nodes or the spleen ) . stage iv indicates disseminated involvement of @entityextralymphatic_organs and includes @entitylymphoma with liver , bone marrow or @entitynodular_pulmonary_disease . modifiers may be used for @entitystage_subclassification . ‘ e ’ denotes @entityextranodal_disease , ‘ a ’ or ‘ b ’ refers to absence or presence of @entityb_symptoms , ‘ s ’ refers to splenic involvement and ‘ x ’ is used if the largest deposit is & gt ; 10 cm , that is , @entitybulky_disease . the major pitfall associated with the ann arbor staging is that it does not differentiate among different oals based on anatomic location , bilaterality , multicentricity and extent of @entityinfiltration_of_the_primary_tumour . hence , it can not determine prognosis in these cases . aronow et al did a retrospective clinical review of 63 patients with primary oal who were staged according to the ajcc - tnm clinical staging system . they determined that the @entitytnm_system was indeed useful for precise characterisation of the extent of @entitylocal_disease . although the t stage did not predict relapse or @entitysurvival , n1 - 4 and m1 stages were able to determine less favourable survival outcomes . @entitylocalised_conjunctival_lymphoma has a good prognosis . it usually has normal @entityserum_ldh_levels , absence of @entityb_symptoms and follows an indolent course . twenty per cent of patients with @entitylocalised_disease will develop @entitydisseminated_disease . less than 5 % of patients die from the @entitylymphoma . unfavourable prognostic indicators previously identified include advanced age at diagnosis , female , prior history of @entitylymphoma , higher disease stage , @entityelevated_international_prognostic_index_score , nodal involvement , @entitynon-conjunctival_primary_site , bilaterality at presentation , age older than 60 years , @entityb_symptoms and @entityelevated_ldh_levels_._relapses_usually_involve_the_contralateral_orbit . @entityhigh_-_grade_transformation may occur in 1 – 3 % of cases . treatment depends on whether the disease is localised or disseminated . @entitylocalised_disease may be treated with @entityebrt , @entityintralesional_injections of @entityinterferon_alpha 2b or @entityrituximab , or @entitylocal_excision . 5 @entitysurgical_resection may be associated with a @entityhigh_risk_of_recurrence due to presence of @entitytumour_microinvasion in the surrounding tissues . elderly and @entityfrail_patients or those with @entitysignificant_comorbidities and @entityasymptomatic_disease may be offered a ‘ wait - and - watch ’ strategy . ebrt has a @entityhigh_response_rate , good local control and a 4 - year relapse rate of 20 – 25 % . russell et al10 showed that a median radiation dose of 30 gy led to excellent local control and was well tolerated with @entityexpected_cataractogenesis . patients with @entityrecurrence were also salvaged successfully . the entire conjunctiva on the involved site should be irradiated . this is because conjunctiva is a lymphoid - rich tissue compared with @entityother_structures_in_the_orbit . if it is not completely included in the radiation field , it may lead to @entitylocal_relapses . complications of ebrt include @entitydry_eye , @entitycataract , @entityglaucoma , @entityretinal_bleeding and @entityretinopathy . @entityradiation_-_induced_retinopathy and @entityretinal_bleeding usually occur with @entityradiation_doses above 40 gy . @entitydisseminated_disease is treated with either @entitychemotherapy ( eg , @entitycyclophosphamide , @entitydoxorubicin , @entityvincristine and @entityprednisone , ie , @entitychop with or without @entityrituximab ) , @entityimmunotherapy ( rituximab ) or @entityradioimmunotherapy ( using @entity90y_-_ibritumomab_tiuxetan ) . rituximab is a @entitychimeric_monoclonal_antibody targeted against the cd20 antigen on @entityb_lymphocytes . 90yttrium - @entityibritumomab_tiuxetan as a modality of @entitytargeted_radiotherapy_offers_the_advantage of delivering a lower radiation dose than ebrt and at the same time targeting @entitymalignant_cells_throughout_the_body . it can be used for @entityrituximab @entityrefractory_disease . this is because the two of them have different mechanisms of action . while @entityrituximab utilises host effector mechanisms to kill @entitytumour_cells , @entity90yttrium_-_ibritumomab_tiuxetan_acts directly through emission of @entitybeta_particles . the eradication of @entityc._psittaci_infection with @entitydoxycycline ( 100 mg administered orally twice a day , for 3 weeks ) has been proposed as an effective strategy but remains controversial . 6\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sent, dot=True):\n",
    "    if dot:\n",
    "        sent = sent + ' .'\n",
    "    text = '[CLS] {} [SEP]'.format(sent)\n",
    "    tokenized_text = tokenizer.tokenize(text)\n",
    "    indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "    # not taking the masked tokens into account\n",
    "\n",
    "    segments_ids = [0]*len(indexed_tokens)\n",
    "    # Convert inputs to PyTorch tensors\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    segments_tensors = torch.tensor([segments_ids])\n",
    "    with torch.no_grad():\n",
    "        # See the models docstrings for the detail of the inputs\n",
    "        outputs = model(tokens_tensor, token_type_ids=segments_tensors)\n",
    "        # Transformers models always output tuples.\n",
    "        # See the models docstrings for the detail of all the outputs\n",
    "        # In our case, the first element is the hidden state of the last layer of the Bert model\n",
    "        encoded_layers = outputs[0]\n",
    "        \n",
    "    return indexed_tokens[1:-1], encoded_layers[0, 1:-1, :]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_embedding(doc):\n",
    "    sents = doc.split(' . ')\n",
    "    doc_tokens = []\n",
    "    doc_emb = []\n",
    "    for i, s in enumerate(sents):\n",
    "        if i == len(sents)-1:\n",
    "            tok, emb = get_embedding(s, False)\n",
    "        else:\n",
    "            tok, emb = get_embedding(s)\n",
    "        doc_tokens.extend(tok)\n",
    "        doc_emb.append(emb)\n",
    "    \n",
    "    doc_emb = torch.cat(doc_emb, dim=0)\n",
    "#     print(doc_emb.shape)\n",
    "#     print(len(doc_tokens))\n",
    "    return doc_tokens, doc_emb\n",
    "        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def score(cand, doc_tokens, all_probs_start, all_probs_end):\n",
    "#     \"\"\":param cand: list of tokens in a candidate answer\n",
    "#        :param doc_tokens: list of tokens in the document\n",
    "#        :param allprobs: tensor of probabilities\n",
    "#     \"\"\"\n",
    "#     score = 0\n",
    "#     for i, t in enumerate(doc_tokens):\n",
    "#         j = i+len(cand)-1\n",
    "#         if j < len(doc_tokens) and t == cand[0] and doc_tokens[j] == cand[-1]:\n",
    "#             score += all_probs_start[i]*all_probs_end[j]\n",
    "            \n",
    "#     return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(candidates, map_cands, doc_tokens, all_probs_start, all_probs_end, average=True):\n",
    "    \"\"\":param map_cands: map of first token of a candidate answer to its position in candidates\n",
    "       :param candidates: list of list of tokens in candidate answers\n",
    "       :param doc_tokens: list of tokens in the document\n",
    "       :param allprobs: tensor of probabilities\n",
    "       :return: score of each candidate answer normalized over candidates\n",
    "    \"\"\"\n",
    "    scores = [0]*len(candidates)\n",
    "    counts = [0]*len(candidates)\n",
    "    for i, t in enumerate(doc_tokens):\n",
    "        for c in map_cands.get(t, []):\n",
    "            cand = candidates[c]\n",
    "            j = i+len(cand)-1\n",
    "            if j < len(doc_tokens) and t == cand[0] and doc_tokens[j] == cand[-1]:\n",
    "                scores[c] += (all_probs_start[i]*all_probs_end[j]).item()\n",
    "#                 print(scores)\n",
    "                counts[c] += 1\n",
    "    if average:\n",
    "        return np.array(scores)/np.array(counts)\n",
    "    return np.array(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_answer(answer, query, doc):\n",
    "    \"\"\":param answer: potential answer\n",
    "       :param query:\n",
    "       :param doc: doc with entities marked\n",
    "    \"\"\"\n",
    "    if len(answer.split()) > 1: #multiple entity\n",
    "        return answer\n",
    "    \n",
    "    abbreviation = '( {} )'.format(answer)\n",
    "    if abbreviation in query: #need to find better answer\n",
    "        define_ind = doc.index(abbreviation)\n",
    "        prev_words = doc[:define_ind].split()\n",
    "        for j in range(len(prev_words)-1, -1, -1):\n",
    "            if prev_words[j].startswith(\"@entity\"):\n",
    "                return ent_to_plain(prev_words[j])\n",
    "            \n",
    "    return answer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answers(document, query, candidates):\n",
    "    # get query embedding\n",
    "#     query = query.replace('▶ ', '').replace('@placeholder', '[MASK]')\n",
    "#     query_tokens, query_emb = get_embedding(query)\n",
    "#     mask_ind = query_tokens.index(tokenizer.convert_tokens_to_ids(['[MASK]'])[0])\n",
    "#     mask_emb = query_emb[mask_ind:mask_ind+1, :]\n",
    "\n",
    "    query = query.replace('▶ ', '').replace('@placeholder', '[MASK] [MASK]')\n",
    "    query_tokens, query_emb = get_embedding(query)\n",
    "    mask_ind_start = query_tokens.index(tokenizer.convert_tokens_to_ids(['[MASK]'])[0])\n",
    "    mask_emb_start = query_emb[mask_ind_start:mask_ind_start+1, :]\n",
    "    mask_ind_end = query_tokens.index(tokenizer.convert_tokens_to_ids(['[MASK]'])[0])+1\n",
    "    mask_emb_end = query_emb[mask_ind_end:mask_ind_end+1, :]\n",
    "        \n",
    "    # get document embeddings\n",
    "    doc_tokens, doc_emb = doc_embedding(ent_to_plain_doc(document))\n",
    "    \n",
    "    dot_product_start = torch.mm(mask_emb_start, torch.transpose(doc_emb, 0, 1))\n",
    "    dot_product_end = torch.mm(mask_emb_end, torch.transpose(doc_emb, 0, 1))\n",
    "    all_probs_start = torch.nn.functional.softmax(dot_product_start, dim = 1).reshape(-1)\n",
    "    all_probs_end = torch.nn.functional.softmax(dot_product_end, dim = 1).reshape(-1)\n",
    "    \n",
    "    # get the candidate answers position and embeddings\n",
    "    cand_ans = [tokenizer.convert_tokens_to_ids(tokenizer.tokenize(c)) for c in candidates]\n",
    "    \n",
    "    maps_cand = collections.defaultdict(list)\n",
    "    for i, ca in enumerate(cand_ans):\n",
    "        maps_cand[ca[0]].append(i)\n",
    "            \n",
    "#     print(maps_cand.get(2569, []))\n",
    "    cand_ans_prob = score(cand_ans, maps_cand, doc_tokens, all_probs_start, all_probs_end, True)\n",
    "    \n",
    "#     cand_ans_prob = [score(c, doc_tokens, all_probs_start, all_probs_end) for c in cand_ans]\n",
    "\n",
    "\n",
    "#     print(cand_ans_prob)\n",
    "    ans_ind = np.argmax(cand_ans_prob)\n",
    "#     print(ans_ind)\n",
    "    \n",
    "    answer = candidates[ans_ind]\n",
    "    \n",
    "    return full_answer(answer, query, document)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc_tok, doc_emb = doc_embedding(document)\n",
    "# print(len(doc_tok))\n",
    "# print(doc_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 73/4147 [06:50<8:00:13,  7.07s/it]"
     ]
    }
   ],
   "source": [
    "towrite = []\n",
    "for pt in tqdm(sample_data):\n",
    "    document, query, candidates, answer = pt['p'], pt['q'], pt['c'], pt['a']\n",
    "    predicted = get_answers(document, query, candidates)\n",
    "    towrite.append('{}::{}\\n'.format(predicted, answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/test_averaged.txt', 'w') as f:\n",
    "    f.write(''.join(towrite))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
